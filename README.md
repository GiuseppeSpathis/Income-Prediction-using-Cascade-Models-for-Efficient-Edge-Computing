# Income Prediction with Cascade Inference Models

## Project Description
This project explores the implementation and analysis of cascade inference systems designed to optimize machine learning deployment in edge computing environments. The primary objective is to balance the trade-off between inference accuracy and resource consumption, specifically addressing latency, energy, and bandwidth constraints. By utilizing a multi-stage architecture, the system employs a lightweight, fast model to handle simple queries directly on the edge device while offloading complex, low-confidence predictions to a larger, more accurate model residing on a remote server. This approach minimizes unnecessary communication and computation overhead without compromising overall predictive performance.

## Methodology
The core of the solution involves probabilistic regression techniques to estimate model confidence. We implemented a cascade system comprising a small Probabilistic Linear Model and a large Probabilistic Multi-Layer Perceptron (MLP). Unlike standard regression models that output a single scalar value, our models were designed to predict a probability distribution (mean and standard deviation) or specific quantiles. This allows the system to quantify uncertainty dynamically. The workflow operates by first querying the small model; if the predicted uncertainty exceeds a calibrated threshold, the input is deemed "hard" and is offloaded to the large model. We utilized the Folktables Income dataset for training and validation, adapting various loss functions including Mean Squared Error, Mean Absolute Error, and Huber Loss to support uncertainty quantification via Negative Log-Likelihood minimization.

## Results and Analysis
Our analysis reveals distinct behaviors in cascade efficiency depending on the complexity of the data and the capacity of the small model. While the Gaussian Negative Log-Likelihood method provided a functional basis for uncertainty estimation, we observed that the linear model served as a relatively weak filter for the complex income dataset. This resulted in a near-linear relationship between resource usage and error reduction, suggesting that for high-variance data, the small model often lacks the capacity to confidently distinguish between easy and hard cases.

To address the limitations of Gaussian assumptions on skewed data, we implemented Quantile Regression. By defining confidence as the interval width between predicted quantiles, this method offered a robust, distribution-free alternative for detecting uncertainty, which aligned better with the underlying data distribution. Additionally, we explored model specialization by training the large model exclusively on high-uncertainty instances identified by the small model. The results highlighted a critical trade-off where specialized models lost global context, often failing to outperform general-purpose models on the full test set. These findings underscore the importance of selecting appropriate uncertainty metrics and the challenges of specialized training in cascade architectures.

## Authors
Salma Doubali, Souad Aqbali, Giuseppe Spathis
